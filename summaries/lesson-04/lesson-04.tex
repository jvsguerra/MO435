% !TeX spellcheck = en_US
% !BIB program = biber 
\documentclass{article}

%% Encoding
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%% Fonts
% Math fonts (fourier) with utopia (erewhon) text fonts
\usepackage{fourier, erewhon}

%% Setup
% This package contains logos
% \usepackage[autoload]{adn}

% \setlogos[
% \textbf{MO435 --- Probabilistic Machine Learning}\\%[5pt]
% \uppercase{Instituto de Computação --- UNICAMP}\\%[-7pt]
% ]%
% {IC3D}%
% {UNICAMP}

%% Transform section references
\makeatletter
\renewcommand*{\p@section}{\S\,}
\renewcommand*{\p@subsection}{\S\,}
\makeatother

%% Shorthands
\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{e.g\onedot} \def\Eg{E.g\onedot}
\def\ie{i.e\onedot} \def\Ie{I.e\onedot}
\def\cf{cf\onedot} \def\Cf{Cf\onedot}
\def\etc{etc\onedot} \def\vs{vs\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{et al\onedot}
\makeatother

%%%
% Other packages start here (see the examples below)
%%

%% Figues
\usepackage{graphicx}
\graphicspath{{./images/}}


%% References
% Use this section to embed your bibliography
% Instead of having a separate file, just place the bibtex entries here
\usepackage{filecontents}% create files
\begin{filecontents}{\jobname.bib}
  @article{Burt1983,
    title={A multiresolution spline with application to image mosaics},
    author={Burt, Peter J and Adelson, Edward H},
    journal={ACM Transactions on Graphics (TOG)},
    volume={2},
    number={4},
    pages={217--236},
    year={1983},
    publisher={ACM},
    url={http://persci.mit.edu/pub_pdfs/spline83.pdf}
  }
\end{filecontents}
% Include bibliography file
\usepackage[
backend=biber,
style=ieee, 
natbib=true,
]{biblatex}
\addbibresource{\jobname.bib}


%% Math
\usepackage{amsmath}


%% Enumerate
\usepackage{enumitem}


\begin{document}
% Put the topic of the assignment here, e.g., 'Linear Filtering' or 'Convolution and filters'
\title{Generative models for discrete data\\\normalsize Lesson No. 4}
% Put your name here 
\author{Jo\~ao Victor da Silva Guerra}

\maketitle

\section{Introduction - notes}

A generative classifier has the following form:

\begin{equation}
  p(y=c|x,\theta) \propto p(x|y=c,\theta)p(y=c,\theta)
\end{equation}

The key to using such models is specifying a suitable form for the class-conditional density $p(x|y=c,\theta)$, which defines what kind of data we expect in each class. Also, we discuss how to infer the unkown parameters $\theta$.

\section{Bayesian concept of Learning - notes}

Physicological research has shown that people can learn concepts from positive examples (e.g. "this is a car", "this is a pen", \etc).

\textbf{Concept learning}, also known category learning, concept attainment, and concept formation, are the search for and listing of features (attributes) that helps us to classify data, and each member of a class has a set of common relevant features. If we include uncertainty about our classification, we can emulate \textbf{fuzzy set theory}, but using standard probability calculus.

Given a set of observations $\mathcal{D}$, a classic approach to induction is to suppose we have a \textbf{hypothesis space} of concepts, $\mathcal{H}$, such as: odd numbers, even numbers, all numbers between 1 and 100, \etc. The subset of $\mathcal{H}$ that is consistent with the data $\mathcal{D}$ is called \textbf{version space}. As we increase out set of observations, the version space shrinks and we become increasingly certain about the concept.

\subsubsection{Likelihood}

Why we chose a hypothesis, when more than one is consistent with the observations? The key intuition is that we want to avoid \textbf{suspicious coincidences}. 
\newline

\texttt{Example:}
Given a set $\mathcal{D} = \{16,8,2,64\}$, if the true concept was even numbers, how come we only saw numbers that happened to be powers of two?

To formalize this, let us assume that examples are sampled uniformly at random from the extension of a concept (the extension of a concept is just the set of number that belong to it, \eg, the extension of $h_{even} = \{2,4,6,...,98,100\}$).
Tenenbaum (\textbf{ref} p.66 MLAPP) describes this the \textbf{strong sampling assumption}.

Given this assumption, the probability of independently sampling $N$ items (with replacement) from $h$ is given by:

\begin{equation}
  p(\mathcal{D}|h) = \left[\dfrac{1}{size(h)}\right]^N = \left[\dfrac{1}{|h|}\right]^N
\end{equation}

This crucial equation embodies what Tenenbaum calls the \textbf{size principle} (or, commonly \textbf{Occam's razor}), which means the model favors the simplest (smallest) hypothesis consistent with the data.






\printbibliography

\end{document}
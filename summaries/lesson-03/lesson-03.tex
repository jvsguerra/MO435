% !TeX spellcheck = en_US
% !BIB program = biber 
\documentclass{article}

%% Encoding
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

%% Fonts
% Math fonts (fourier) with utopia (erewhon) text fonts
\usepackage{fourier, erewhon}

%% Setup
% This package contains logos
% \usepackage[autoload]{adn}

% \setlogos[
% \textbf{MO435 --- Probabilistic Machine Learning}\\%[5pt]
% \uppercase{Instituto de Computação --- UNICAMP}\\%[-7pt]
% ]%
% {IC3D}%
% {UNICAMP}

%% Transform section references
\makeatletter
\renewcommand*{\p@section}{\S\,}
\renewcommand*{\p@subsection}{\S\,}
\makeatother

%% Shorthands
\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{e.g\onedot} \def\Eg{E.g\onedot}
\def\ie{i.e\onedot} \def\Ie{I.e\onedot}
\def\cf{cf\onedot} \def\Cf{Cf\onedot}
\def\etc{etc\onedot} \def\vs{vs\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{et al\onedot}
\makeatother

%%%
% Other packages start here (see the examples below)
%%

%% Figues
\usepackage{graphicx}
\graphicspath{{./images/}}


%% References
% Use this section to embed your bibliography
% Instead of having a separate file, just place the bibtex entries here
\usepackage{filecontents}% create files
\begin{filecontents}{\jobname.bib}
  @book{barber2012,
    title={Bayesian Reasoning and Machine Learning},
    author={Barber, D},
    year={2012},
    publisher={Cambridge University Press},
  }
  @book{murphy2012,
    title={Machine Learning: A Probabilistic Perspective},
    author={Murphy, Kevin P},
    pages={27--33},
    year={2012},
    publisher={The MIT Press},
  }
\end{filecontents}
% Include bibliography file
\usepackage[
backend=biber, 
style=ieee, 
natbib=true,
]{biblatex}
\addbibresource{\jobname.bib}


%% Math
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%% Enumerate
\usepackage{enumitem}


\begin{document}
% Put the topic of the assignment here, e.g., 'Linear Filtering' or 'Convolution and filters'
\title{Linear regression\\\normalsize Lesson No. 3}
% Put your name here 
\author{Jo\~ao Victor da Silva Guerra}

\maketitle

\section{Introduction}

Linear regression is a fundamental tool of statistics and supervised machine learning, which models the relationship between two variables by fitting a linear equation ($\vec{\textbf{y}} = A \times \vec{\textbf{x}} + B$). In the example, $\vec{\textbf{x}}$ is the explanatory variable and $\vec{\textbf{y}}$ is the dependent variable.

\section{Model description}

In the simplest case, the linear regression model is described as:

\begin{equation}
  p(y|\vec{x},\vec{\theta}) = \mathcal{N} (y| \vec{w}^T \times \vec{x}, \sigma^2)
\end{equation}

In addition, the linear regression can be made to model non-linear relationships by replacing $\vec{x}$ with some non-linear function of $\vec{x}$ (denoted $\phi(\vec{x})$), also known as \textbf{basis function expansion}. In this scenario, the model is described as:

\begin{equation}
  p(\vec{y}|\vec{x},\theta) = \mathcal{N} (y|\vec{w}^T \times \phi(\vec{x}), \sigma^2)
\end{equation}

Note that the model remains linear in the parameters $\vec{w}$.

\section{Least-squares regression}

The parameters of a statistical model is estimated by the maximum likelihood estimation (MLE), which is defined as follows:

\begin{equation}
\hat{\theta} = \argmax_\theta \log p(\mathcal{D}|\theta)
\end{equation}

Assuming the training examples as independent and identically distributed, we can define the \textbf{log-likelihood ($\ell$)} and its counterpart, \textbf{negative log likelihood (NLL)}, as follows:

\begin{equation}
  \label{LL}
  \ell(\theta) = \log p(\mathcal{D}|\theta) = \sum_{i=1}^{N} \log p(y\textsubscript{i}|\vec{x}\textsubscript{i},\theta)
\end{equation}

\begin{equation}
  \label{NLL}
  NLL(\theta) = - \sum_{i=1}^{N} \log p(y\textsubscript{i}|\vec{x}\textsubscript{i},\theta)
\end{equation}

Note that it is equivalent to maximize \eqref{LL} and minimize \eqref{NLL}; however, the last is usually more convenient, because optimization software packages are commonly designed to find the minima functions, rather than maxima.

Applying MLE's method to the linear equation model with the Gaussian definition inserted as well, the log-likelihood is defined as follows:

\begin{equation}
\begin{split}
\ell(\theta) & = \sum_{i=1}^{N} \log \left[\left(\dfrac{1}{2\pi\sigma^2}\right)^{\dfrac{1}{2}} exp \left( - \dfrac{1}{2\sigma^2}(y_i - \vec{w}^T\vec{x_i})^2 \right) \right] \\
& = - \dfrac{1}{2\sigma^2} RSS(\vec{w}) - \dfrac{N}{2} \log(2\pi\sigma^2)
\end{split}
\end{equation}
where RSS is the \textbf{residual sum of squares}, also known as \textbf{sum of squared erros} and is defined as follows:

\begin{equation}
  RSS(\vec{w}) = \sum_{i=1}^{N} (y_i - \vec{w}^T\vec{x_i})^2
\end{equation}

Based on the above equations, the MLE for $\vec{w}$ is the one that minimizes the RSS.

\subsection{Ordinary least squares (???) - incomplete}

The corresponding solution of $\hat{w}$ to this linear system of equations is called the ordinary least squares (OLS), which is defined as:

\begin{equation}
  \hat{w}_{OLS} = (X^{T}X)^{-1}X^{T}y
\end{equation}

\subsection{Convexity}

Firstly, we define a \textbf{convex set ($S$)} as follows:

\begin{equation}
\lambda x + (1 - \lambda) y,
\begin{cases} 
  \text{$\forall \lambda \in [0,1]$} \\ 
  \text{$\forall x, y \in S$} 
\end{cases}
\end{equation}

A function $f(x)$ is convex if its epigraph (set of points above the function) defines a convex set. Equivalently, a function $f(x)$ is convex if it is defined on a convex set as follows:

\begin{equation}
f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y), 
\begin{cases} 
  \text{$\forall \lambda \in [0,1]$} \\ 
  \text{$\forall x, y \in S$} 
\end{cases}
\end{equation}

% --------

\subsection{Study information - can be removed}
* \textbf{trace (often $tr$)} of a square matrix A is defined by the sum of elements on the main diagonal of A.

\begin{equation}
  x^{T}Ax = tr(x^{T}Ax)
\end{equation}

So, for derivation of MLE (p. 220 of \cite{murphy2012})

% --------

\printbibliography

\end{document}